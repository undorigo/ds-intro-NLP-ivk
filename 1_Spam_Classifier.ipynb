{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's build a spam classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use data from `SMS Spam Collection v. 1` described as:\n",
    "\n",
    "> a public set of SMS labeled messages that have been collected for mobile phone spam research. It has one collection composed by 5,574 English, real and non-encoded messages, tagged according being legitimate (ham) or spam.\n",
    "\n",
    "([source](http://www.dt.fee.unicamp.br/~tiago/smsspamcollection/))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load useful libraries and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/ivokafemann/nltk_data...\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/ivokafemann/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data = pd.read_csv(\n",
    "    \"data/SMSSpamCollection.txt\",\n",
    "    encoding=\"utf-8\",\n",
    "    header=None,\n",
    "    delimiter=\"\\t\",\n",
    "    names=[\"target\", \"text\"],\n",
    ")\n",
    "\n",
    "# Encoding target variable\n",
    "data[\"target\"] = np.where(data[\"target\"] == \"spam\", 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>0</td>\n",
       "      <td>I am going to sao mu today. Will be done only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0</td>\n",
       "      <td>Hello! How's you and how did saturday go? I wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>0</td>\n",
       "      <td>K, text me when you're on the way</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     target                                               text\n",
       "124       0  I am going to sao mu today. Will be done only ...\n",
       "39        0  Hello! How's you and how did saturday go? I wa...\n",
       "141       0                  K, text me when you're on the way"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Looking at a sample of our data\n",
    "data.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A quick look at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset contains 5572 instances of 2 variables.\n",
      "It contains 747 spam messages (13.4% of all).\n"
     ]
    }
   ],
   "source": [
    "print(\"Dataset contains {} instances of {} variables.\".format(data.shape[0], data.shape[1]))\n",
    "\n",
    "print(\n",
    "    \"It contains {} spam messages ({:.1%} of all).\".format(\n",
    "        data[data.target == 1].shape[0],\n",
    "        data[data.target == 1].shape[0] / data.shape[0],\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples of spam SMS: \n",
      "    I don't know u and u don't know me. Send CHAT to 86688 now and let's find each other! Only 150p/Msg rcvd. HG/Suite342/2Lands/Row/W1J6HL LDN. 18 years or over.\n",
      "    For sale - arsenal dartboard. Good condition but no doubles or trebles!\n",
      "\n",
      "Examples of non-spam SMS: \n",
      "    AH POOR BABY!HOPE URFEELING BETTERSN LUV! PROBTHAT OVERDOSE OF WORK HEY GO CAREFUL SPK 2 U SN LOTS OF LOVEJEN XXX.\n",
      "    Ü takin linear algebra today?\n"
     ]
    }
   ],
   "source": [
    "## Printing random samples of text from both the classes i.e. Spam and non-Spam\n",
    "print(\n",
    "    \"Examples of spam SMS: \\n    {}\\n    {}\".format(\n",
    "        data[data.target == 1].sample(1).text.iloc[0],\n",
    "        data[data.target == 1].sample(1).text.iloc[0],\n",
    "    )\n",
    ")\n",
    "print(\n",
    "    \"\\nExamples of non-spam SMS: \\n    {}\\n    {}\".format(\n",
    "        data[data.target == 0].sample(1).text.iloc[0],\n",
    "        data[data.target == 0].sample(1).text.iloc[0],\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spam classification\n",
    "\n",
    "We will here build a \"vanilla\" classifier, without pouring too many thoughts about what the actual messages, spam or not, look like. To improve your model you can of course have a closer look and investigate the data more in detail. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset between train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data[\"text\"], data[\"target\"], random_state=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "872     Its going good...no problem..but still need li...\n",
       "831     U have a secret admirer. REVEAL who thinks U R...\n",
       "1273                                                Ok...\n",
       "3314    Huh... Hyde park not in mel ah, opps, got conf...\n",
       "4929    Just hopeing that wasn‘t too pissed up to reme...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CountVectorizer\n",
    "\n",
    "As you have seen, our X variable is just plain text == a string. No classifier can handle it, so we need to make the text accessible to the model. Therefore, we can transform the text so that each word is a separate feature and we count how many times that word occurs in the SMS. We can do this with the scikit-learn [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html).  It will convert our text and return a sparse matrix. The vocabulary space of an English text is quite large, while in an SMS you will use only a small subset of words. Therefore saving this feature matrix as a sparse matrix will save memory space.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_vectorized:  <Compressed Sparse Row sparse matrix of dtype 'int64'\n",
      "\twith 55484 stored elements and shape (4179, 7546)>\n",
      "  Coords\tValues\n",
      "  (0, 915)\t1\n",
      "  (0, 1526)\t1\n",
      "  (0, 2058)\t1\n",
      "  (0, 2630)\t1\n",
      "  (0, 3090)\t1\n",
      "  (0, 3100)\t1\n",
      "  (0, 3676)\t1\n",
      "  (0, 4049)\t1\n",
      "  (0, 4593)\t1\n",
      "  (0, 4663)\t1\n",
      "  (0, 5294)\t1\n",
      "  (0, 6315)\t1\n",
      "  (0, 6764)\t1\n",
      "  (0, 6964)\t1\n",
      "  (0, 7133)\t1\n",
      "  (1, 36)\t1\n",
      "  (1, 216)\t1\n",
      "  (1, 532)\t1\n",
      "  (1, 801)\t1\n",
      "  (1, 1553)\t1\n",
      "  (1, 1598)\t1\n",
      "  (1, 2055)\t1\n",
      "  (1, 3267)\t1\n",
      "  (1, 4493)\t1\n",
      "  (1, 4835)\t1\n",
      "  :\t:\n",
      "  (4176, 6904)\t1\n",
      "  (4176, 6916)\t1\n",
      "  (4176, 7019)\t1\n",
      "  (4176, 7333)\t1\n",
      "  (4176, 7367)\t2\n",
      "  (4176, 7449)\t1\n",
      "  (4177, 1575)\t1\n",
      "  (4177, 2422)\t1\n",
      "  (4177, 3081)\t1\n",
      "  (4177, 3424)\t1\n",
      "  (4177, 3505)\t1\n",
      "  (4177, 3551)\t1\n",
      "  (4177, 4306)\t1\n",
      "  (4177, 4538)\t1\n",
      "  (4177, 5739)\t1\n",
      "  (4177, 7228)\t2\n",
      "  (4178, 2121)\t1\n",
      "  (4178, 2717)\t1\n",
      "  (4178, 3040)\t1\n",
      "  (4178, 3677)\t1\n",
      "  (4178, 4410)\t1\n",
      "  (4178, 5901)\t1\n",
      "  (4178, 6070)\t1\n",
      "  (4178, 6644)\t1\n",
      "  (4178, 6771)\t1\n"
     ]
    }
   ],
   "source": [
    "# Fit the CountVectorizer to the training data\n",
    "vect = CountVectorizer().fit(X_train)\n",
    "\n",
    "# transform the documents in the training data to a document-term matrix\n",
    "X_train_vectorized = vect.transform(X_train)\n",
    "print(\"X_train_vectorized: \", X_train_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape = (4179,)\n",
      "Vocabulary length = 7546\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train shape = {}\".format(X_train.shape))\n",
    "print(\"Vocabulary length = {}\".format(len(vect.vocabulary_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So in 4179 messages we found 7546 different words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('00', 0),\n",
       " ('000', 1),\n",
       " ('000pes', 2),\n",
       " ('008704050406', 3),\n",
       " ('0089', 4),\n",
       " ('0121', 5),\n",
       " ('01223585334', 6),\n",
       " ('0125698789', 7),\n",
       " ('02', 8),\n",
       " ('0207', 9),\n",
       " ('02072069400', 10),\n",
       " ('02073162414', 11),\n",
       " ('021', 12),\n",
       " ('03', 13),\n",
       " ('04', 14),\n",
       " ('0430', 15),\n",
       " ('05', 16),\n",
       " ('050703', 17),\n",
       " ('0578', 18),\n",
       " ('06', 19)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's look at our vocabulary list (sorted alphabetically)\n",
    "# Does it look like you expected?\n",
    "sorted(vect.vocabulary_.items(), key=lambda x: x[1])[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# We can also print the newly created feature matrix\n",
    "# Note: you see its a sparse matrix with many 0 values. \n",
    "# with .toarray() the compressed sparse matrix form is converted to a normal numpy array\n",
    "print(X_train_vectorized.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can train our first model with the vectorized data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC = 0.985\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "model = LogisticRegression(max_iter=1500)\n",
    "model.fit(X_train_vectorized, y_train)\n",
    "\n",
    "# Predict the transformed test documents\n",
    "predictions = model.predict(vect.transform(X_test))\n",
    "predict_probab = model.predict_proba(vect.transform(X_test))[:,1]\n",
    "\n",
    "print(\"AUC = {:.3f}\".format(roc_auc_score(y_test, predict_probab)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see which words have the highest and lowest coefficients.\n",
    "\n",
    "Think back to the sigmoid function (logistic function). \n",
    "What class are observations assigned to if they contain words with high coefficients?  And to which class are they assigned if they contain words with high negative coefficients?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smallest Coefs:\n",
      "['me' 'my' 'gt' 'lt' 'll' 'but' 'am' 'he' 'amp' 'right']\n",
      "\n",
      "Largest Coefs: \n",
      "['txt' 'uk' 'ringtone' 'text' 'call' 'chat' 'reply' 'new' 'won' 'stop']\n"
     ]
    }
   ],
   "source": [
    "# get the feature names as numpy array\n",
    "feature_names = np.array(vect.get_feature_names_out())\n",
    "\n",
    "# Sort the coefficients from the model (from lowest to highest values)\n",
    "sorted_coef_index = model.coef_[0].argsort()\n",
    "\n",
    "# Find the 10 smallest and 10 largest coefficients\n",
    "# The 10 largest coefficients are being indexed using [:-11:-1]\n",
    "# so the list returned is in order of largest to smallest\n",
    "print(\"Smallest Coefs:\\n{}\\n\".format(feature_names[sorted_coef_index[:10]]))\n",
    "print(\"Largest Coefs: \\n{}\".format(feature_names[sorted_coef_index[:-11:-1]]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The AUC of our first model was already pretty good (~0.95). Let's see if we can improve this with another transformation of our data. Therefore, we will test the TF-IDF transformation next.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### TF-IDF\n",
    "\n",
    "TF-IDF is short for **Term Frequency - Inverse Document Frequency**. \n",
    "\n",
    "It measure how important a word is to a document in a set of texts (in our case all SMS we collected). A frequent word in a document that is also frequent in the corpus is less important to a document than a frequent word in a document that is not frequent in the corpus.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('2000', 10),\n",
       " ('250', 11),\n",
       " ('2nd', 12),\n",
       " ('50', 13),\n",
       " ('500', 14),\n",
       " ('5000', 15),\n",
       " ('800', 16),\n",
       " ('able', 17),\n",
       " ('about', 18),\n",
       " ('abt', 19),\n",
       " ('account', 20),\n",
       " ('actually', 21),\n",
       " ('address', 22),\n",
       " ('after', 23),\n",
       " ('afternoon', 24),\n",
       " ('again', 25),\n",
       " ('ah', 26),\n",
       " ('aight', 27),\n",
       " ('all', 28),\n",
       " ('alone', 29)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the TfidfVectorizer to the training data specifiying a minimum document frequency of 15\n",
    "# This means a word should have been used in at least 15 SMS \n",
    "vect = TfidfVectorizer(min_df=15).fit(X_train)\n",
    "\n",
    "# transform the documents in the training data to a document-term matrix\n",
    "X_train_vectorized = vect.transform(X_train)\n",
    "\n",
    "# let's look of some of the words gathered with this method\n",
    "sorted(vect.vocabulary_.items(), key=lambda x: x[1])[10:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "577"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how many words appear in more than 15 text messages\n",
    "len(sorted(vect.vocabulary_.items(), key=lambda x: x[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check which words created the largest tfidf values for the texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smallest tfidf:\n",
      "['2000' 'weekly' 'rate' '16' 'sae' 'await' 'vouchers' 'guaranteed' '1000'\n",
      " 'collection']\n",
      "\n",
      "Largest tfidf: \n",
      "['yup' 'with' 'sure' 'babe' 'heart' 'he' 'thank' 'why' 'happy' 'thanx']\n"
     ]
    }
   ],
   "source": [
    "# save all feature names == words in an array\n",
    "feature_names = np.array(vect.get_feature_names_out())\n",
    "\n",
    "#sort for the column names according to highest tfidf value in the column\n",
    "sorted_tfidf_index = X_train_vectorized.toarray().max(0).argsort()\n",
    "\n",
    "# print words with highest and lowest tfidf values\n",
    "print(\"Smallest tfidf:\\n{}\\n\".format(feature_names[sorted_tfidf_index[:10]]))\n",
    "print(\"Largest tfidf: \\n{}\".format(feature_names[sorted_tfidf_index[:-11:-1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use our new features with logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC = 0.988\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "model = LogisticRegression(max_iter=1500)\n",
    "model.fit(X_train_vectorized, y_train)\n",
    "\n",
    "# Predict the transformed test documents\n",
    "predictions = model.predict_proba(vect.transform(X_test))[:,1]\n",
    "\n",
    "print(\"AUC = {:.3f}\".format(roc_auc_score(y_test, predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With only 577 features out of 7546 (7546 different words were in our training texts), we still get a high value for the AUC score.\n",
    "Feel free to test different values for the minimum document frequency for the tf-idf vectorizer and see how this affects the model.\n",
    "\n",
    "Again, we can look at the coefficients of our new model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smallest Coefs:\n",
      "['me' 'my' 'gt' 'lt' 'll' 'that' 'ok' 'later' 'da' 'how']\n",
      "\n",
      "Largest Coefs: \n",
      "['txt' 'call' 'text' 'free' 'stop' 'uk' 'claim' 'www' 'reply' '150p']\n"
     ]
    }
   ],
   "source": [
    "# Sort the coefficients from the model\n",
    "sorted_coef_index = model.coef_[0].argsort()\n",
    "\n",
    "# Find the 10 smallest and 10 largest coefficients\n",
    "# The 10 largest coefficients are being indexed using [:-11:-1]\n",
    "# so the list returned is in order of largest to smallest\n",
    "print(\"Smallest Coefs:\\n{}\\n\".format(feature_names[sorted_coef_index[:10]]))\n",
    "print(\"Largest Coefs: \\n{}\".format(feature_names[sorted_coef_index[:-11:-1]]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that there is a lot of overlap in the features that received the highest and lowest coefficients compared to the previous model; regardless of how we convert our text into features, these words seem to be important for classifying spam with logistic regression.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text data can be more preprocessed before being used as features in a model. We will first use stemming as an approach:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "\n",
    "Stemming reduces a word to its stem. The result is less readable to humans, but makes the text more comparable across observations.\n",
    "\n",
    "For example, the words \"consult\", \"consultant\", \"consulting\", \" consultative\", \"consultants\" have the same stem **\"consult \"**.\n",
    "\n",
    "We will now add stemming as a preprocessing step to our workflow. The nltk PorterStemmer will generate the stems of the words. These features will be used in the CountVectorizer to create a matrix with the number of features (stemmed words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing stemmer and countvectorizer \n",
    "stemmer = nltk.PorterStemmer()\n",
    "cv_analyzer = CountVectorizer().build_analyzer()\n",
    "# tfidf_analyzer = TfidfVectorizer(min_df=15).build_analyzer()\n",
    "\n",
    "def stemmed_words(doc):\n",
    "    return (stemmer.stem(w) for w in cv_analyzer(doc))\n",
    "\n",
    "# define CountVectorizer with stemming function \n",
    "stem_vectorizer = CountVectorizer(analyzer = stemmed_words)\n",
    "# stem_vectorizer = TfidfVectorizer(min_df=15, analyzer = stemmed_words)\n",
    "\n",
    "\n",
    "# Transform X_train\n",
    "X_train_stem_vectorized = stem_vectorizer.fit_transform(X_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To break the above code cell into steps and show what steps are doing what and why are we doing this overall\n",
    "\n",
    "- The function `build_analyzer()` of `CountVectorizer()` handles the pre-processing, tokenizing and n-grams generation for the text\n",
    "- In the function `stemmed_words()` \n",
    "  - The text is first passed through the `build_analyzer()` and then each word in the text is stemmed to its base form\n",
    "- This whole thing is called with the last step when we call `fit_transform()` on the dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In the below cell we can see how `build_analyzer()` pre-processes the sample text and tokenize it\n",
    "- And at the last line, the stemmer stems each word in the text to its base form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Text -  Its going good...no problem..but still need little experience to understand american customer voice...\n",
      "------------------------------\n",
      "Text after passing through build_analyzer -  ['its', 'going', 'good', 'no', 'problem', 'but', 'still', 'need', 'little', 'experience', 'to', 'understand', 'american', 'customer', 'voice']\n",
      "------------------------------\n",
      "Text after stemming -  ['it', 'go', 'good', 'no', 'problem', 'but', 'still', 'need', 'littl', 'experi', 'to', 'understand', 'american', 'custom', 'voic']\n"
     ]
    }
   ],
   "source": [
    "sample_text = X_train[:1]\n",
    "print(\"Sample Text - \", sample_text[872])\n",
    "print(\"-\"*30)\n",
    "print(\"Text after passing through build_analyzer - \", cv_analyzer(sample_text[872]))\n",
    "print(\"-\"*30)\n",
    "print(\"Text after stemming - \",[stemmer.stem(w) for w in cv_analyzer(sample_text[872])])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also try uncommenting the tfidf lines in the cell above, so instead of using CountVectorizer you can also use TfIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC = 0.984\n"
     ]
    }
   ],
   "source": [
    "# Train the model with stemmed and vectorized dataset\n",
    "model_stemm = LogisticRegression(max_iter=1500)\n",
    "model_stemm.fit(X_train_stem_vectorized, y_train)\n",
    "\n",
    "# Predict the transformed test documents\n",
    "predictions = model_stemm.predict_proba(stem_vectorizer.transform(X_test))[:,1]\n",
    "\n",
    "print(\"AUC = {:.3f}\".format(roc_auc_score(y_test, predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smallest Coefs:\n",
      "['me' 'my' 'gt' 'lt' 'll' 'but' 'am' 'hope' 'he' 'that']\n",
      "\n",
      "Largest Coefs: \n",
      "['txt' 'uk' 'rington' 'text' 'chat' 'new' 'repli' 'won' 'call' 'cost']\n"
     ]
    }
   ],
   "source": [
    "# get the feature names as numpy array\n",
    "feature_names = np.array(stem_vectorizer.get_feature_names_out())\n",
    "\n",
    "# Sort the coefficients from the model\n",
    "sorted_coef_index = model_stemm.coef_[0].argsort()\n",
    "\n",
    "# Find the 10 smallest and 10 largest coefficients\n",
    "# The 10 largest coefficients are being indexed using [:-11:-1]\n",
    "# so the list returned is in order of largest to smallest\n",
    "print(\"Smallest Coefs:\\n{}\\n\".format(feature_names[sorted_coef_index[:10]]))\n",
    "print(\"Largest Coefs: \\n{}\".format(feature_names[sorted_coef_index[:-11:-1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we see some other words in the features with absolute highest coefficients.\n",
    "The AUC-score of classification is between the scores of our last two text representation attempts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization\n",
    "\n",
    "The same way we used stemming we can also apply lemmatization to our data.\n",
    "Lemmatization reduces variant forms to base form (eg. am, are, is --> be; car, cars, car's, cars' --> car).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization\n",
    "# initialize a lemmatizer to convert words to their base form\n",
    "WNlemma = nltk.WordNetLemmatizer()\n",
    "# Use built-in analyzer of CountVectorizer to tokenize, lowercase the text and remove punctuation\n",
    "cv_analyzer = CountVectorizer().build_analyzer()\n",
    "\n",
    "# cv_analyzer = TfidfVectorizer(min_df=15).build_analyzer()\n",
    "\n",
    "# define a custom function that lemmatizes the words in the text using the lemmatizer and the analyzer\n",
    "def lemmatize_word(doc):\n",
    "    return (WNlemma.lemmatize(t) for t in cv_analyzer(doc))\n",
    "\n",
    "# create a CountVectorizer with the custom lemmatization function as the analyzer to build the vocabulary \n",
    "# (from lemmas instead of raw words) and vectorize the text\n",
    "lemm_vectorizer = CountVectorizer(analyzer = lemmatize_word)\n",
    "\n",
    "# lemm_vectorizer = TfidfVectorizer(min_df=15, analyzer=lemmatize_word)\n",
    "\n",
    "# Transform X_train\n",
    "X_train_lemm_vectorized = lemm_vectorizer.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipeline:\n",
    "Text → tokenize → lemmatize → count words → numeric matrix\n",
    "\n",
    "Purpose:\n",
    "- Normalize word forms\n",
    "- Reduce vocabulary size\n",
    "- Improve model robustness\n",
    "- Avoid treating “cars” and “car” as separate features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4179, 7091)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_lemm_vectorized.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With lemmatization we were able to reduce the features from ca. 7500 to 7100. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 = 0.909\n",
      "AUC = 0.984\n"
     ]
    }
   ],
   "source": [
    "# Train the model with stemmed and vectorized dataset\n",
    "model_lemm = LogisticRegression(max_iter=1500)\n",
    "model_lemm.fit(X_train_lemm_vectorized, y_train)\n",
    "\n",
    "# Predict the transformed test documents\n",
    "predict_probab = model_lemm.predict_proba(lemm_vectorizer.transform(X_test))[:,1]\n",
    "predictions = model_lemm.predict(lemm_vectorizer.transform(X_test))\n",
    "\n",
    "print(\"F1 = {:.3f}\".format(f1_score(y_test, predictions)))\n",
    "print(\"AUC = {:.3f}\".format(roc_auc_score(y_test, predict_probab)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smallest Coefs:\n",
      "['me' 'my' 'gt' 'lt' 'll' 'but' 'am' 'he' 'then' 'amp']\n",
      "\n",
      "Largest Coefs: \n",
      "['txt' 'uk' 'ringtone' 'text' 'call' 'new' 'reply' 'chat' 'won' 'free']\n"
     ]
    }
   ],
   "source": [
    "# get the feature names as numpy array\n",
    "feature_names = np.array(lemm_vectorizer.get_feature_names_out())\n",
    "\n",
    "\n",
    "# Sort the coefficients from the model\n",
    "sorted_coef_index = model_lemm.coef_[0].argsort()\n",
    "\n",
    "# Find the 10 smallest and 10 largest coefficients\n",
    "# The 10 largest coefficients are being indexed using [:-11:-1]\n",
    "# so the list returned is in order of largest to smallest\n",
    "print(\"Smallest Coefs:\\n{}\\n\".format(feature_names[sorted_coef_index[:10]]))\n",
    "print(\"Largest Coefs: \\n{}\".format(feature_names[sorted_coef_index[:-11:-1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result for this last model are very similar to the first model we tested. \n",
    "\n",
    "You can test how well lemmatization in combination with tf-idf is working on our example data. Just remove the `#` at the beginning of the line (don't forget to add `#` to the respective same lines before)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing our spam classfier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test our classifier. You can also input your own text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'If you were/are free i can give. Otherwise nalla adi entey nattil kittum'"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your input below\n",
    "# input_text = \"We went yesterday to the beach, call me first. But then also call the other guy pls. Use this number and then the other number\"\n",
    "\n",
    "# Or use an example for the test set\n",
    "input_text = X_test.sample(1).iloc[0]\n",
    "input_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a spam!\n"
     ]
    }
   ],
   "source": [
    "# You can change the model with model_stemm or model_lemm \n",
    "if model_lemm.predict(lemm_vectorizer.transform([input_text]))[0] == 1:\n",
    "    print('This is a spam!')\n",
    "else:\n",
    "    print('Not a spam :)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The hamming loss is 0.0230\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import hamming_loss\n",
    "print(f'The hamming loss is {hamming_loss(y_test,predictions):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feel free to test your own SMS messages and see which words you can add to change the prediction of a ham message to a spam message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

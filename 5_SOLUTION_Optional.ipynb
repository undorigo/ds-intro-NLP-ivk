{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Spam Classifier Word Embeddings - Solution\n",
    "\n",
    "Let's have a look at word embeddings. Therefore, we want to \n",
    "\n",
    "1. Apply the [TensorFlow tutorial](https://www.tensorflow.org/tutorials/text/word_embeddings) to the Spam/Ham dataset.\n",
    "\n",
    "2. Load the embedings in the [Embedding Projector](http://projector.tensorflow.org/) to visualize the word embedding.\n",
    "\n",
    "In this notebook you will find one solution how to load and prepare the Spam/Ham data to apply those NLP models."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# importing all needed libraries and functions\n",
    "import io\n",
    "import re\n",
    "import string\n",
    "import tensorflow as tf\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.keras import  Sequential\n",
    "from tensorflow.keras.layers import  Dense, Embedding, GlobalAveragePooling1D\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Loading data with pandas\n",
    "\n",
    "We are loading our Spam and Ham data with pandas and afterwards split our data into a train, a validation and a test set as usual with sklearns train_test_split function.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Load spam/ham data\n",
    "dataframe_full = pd.read_csv(\n",
    "    \"./data/SMSSpamCollection.txt\",\n",
    "    encoding=\"utf-8\",\n",
    "    header=None,\n",
    "    delimiter=\"\\t\",\n",
    "    names=[\"target\", \"text\"],\n",
    ")\n",
    "\n",
    "# Encoding target variable\n",
    "dataframe_full[\"target\"] = np.where(dataframe_full[\"target\"] == \"spam\", 1, 0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# First look at the data\n",
    "dataframe_full.sample(5)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Splitting data in train, validation and test set\n",
    "dataframe_train, dataframe_test = train_test_split(dataframe_full, test_size=0.2, random_state=42)\n",
    "dataframe_train, dataframe_val = train_test_split(dataframe_train, test_size=0.25, random_state=42)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def print_shape(dataframe):\n",
    "    \"\"\"Print number of observations and number of columns of the given dataframe.\n",
    "\n",
    "    Args:\n",
    "        dataframe (pandas DataFrame): Any pandas DataFrame\n",
    "    \"\"\"\n",
    "    name =[x for x in globals() if globals()[x] is dataframe][0]\n",
    "    print(f'There are {dataframe.shape[0]} observations and {dataframe.shape[1]} columns in the {name}.')\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for i in [dataframe_train, dataframe_val, dataframe_test]:\n",
    "    print_shape(i)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Change pandas Dataframes to tf.dataset\n",
    "\n",
    "After spliting the data, we need to transform the data to tensorflow \"tensors\"."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "dataset_train = tf.data.Dataset.from_tensor_slices((dataframe_train.text, dataframe_train.target))\n",
    "dataset_val = tf.data.Dataset.from_tensor_slices((dataframe_val.text, dataframe_val.target))\n",
    "dataset_test = tf.data.Dataset.from_tensor_slices((dataframe_test.text, dataframe_test.target))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "dataset_train = dataset_train.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "dataset_val = dataset_val.cache().prefetch(buffer_size=AUTOTUNE)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Text preprocessing"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def custom_standardization(input_data):\n",
    "    \"\"\"Text preprocessing: lowercases, no punctuation\n",
    "\n",
    "    Args:\n",
    "        input_data (tf.dataframe): [text, formated as tf.string]\n",
    "\n",
    "    Returns:\n",
    "        [tf.dataframe]: [preprocessed text]\n",
    "    \"\"\"\n",
    "    text_lower = tf.strings.lower(input_data)\n",
    "    return tf.strings.regex_replace(text_lower,\n",
    "                                  '[%s]' % re.escape(string.punctuation), '')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Vocabulary size and number of words in a sequence.\n",
    "vocab_size = 7546  # taken from notebook 1\n",
    "sequence_length = int(dataframe_train.text.apply(lambda x: len(x.split())).max())\n",
    "\n",
    "# Use the text vectorization layer to normalize, split, and map strings to \n",
    "# integers. Note that the layer uses the custom_standardization function defined above. \n",
    "# Set maximum_sequence length as all samples are not of the same length.\n",
    "vectorize_layer = TextVectorization(\n",
    "    standardize=custom_standardization,\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=sequence_length)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Make a text-only dataset (without labels), then call adapt\n",
    "train_text = dataset_train.map(lambda x, y: x)\n",
    "vectorize_layer.adapt(train_text)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(\"1287 ---> \",vectorize_layer.get_vocabulary()[1287])\n",
    "print(\" 313 ---> \",vectorize_layer.get_vocabulary()[313])\n",
    "print('Vocabulary size: {}'.format(len(vectorize_layer.get_vocabulary())))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create a classification model\n",
    "\n",
    "In the tutorial the batch-size was defined when loading the data as tf.Dataset. That's why we have to specify this now too. This is especially important for training the model.\n",
    "You can create the batches as shown here: [tf.data.Dataset.batch() method, combined with repeat() method](https://www.gcptutorials.com/article/how-to-use-batch-method-in-tensorflow)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "dataset_train_batch = dataset_train.repeat().batch(batch_size=32)\n",
    "dataset_val_batch = dataset_val.repeat().batch(batch_size=32)\n",
    "dataset_test_batch = dataset_test.repeat().batch(batch_size=32)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# checking shape and type of batched dataset \n",
    "dataset_train_batch"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Defining model structure:**\n",
    "- first vectorize data\n",
    "- then using embedding layer\n",
    "- globalaveragepooling1D layer will return fixed-length output vector even though the input may varry in length\n",
    "- fully connected layer\n",
    "- last layer with single output node"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#model structure\n",
    "embedding_dim=16\n",
    "model = Sequential([\n",
    "  vectorize_layer,\n",
    "  Embedding(vocab_size, embedding_dim, name=\"embedding\"),\n",
    "  GlobalAveragePooling1D(),\n",
    "  Dense(16, activation='relu'),\n",
    "  Dense(1)\n",
    "])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"logs\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# model compiling using Adam optimizer and BinaryCrossentropy loss\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# ploting model loss during training, created by Daniel: https://medium.com/geekculture/how-to-plot-model-loss-while-training-in-tensorflow-9fa1a1875a5\n",
    "class PlotLearning(keras.callbacks.Callback):\n",
    "    \"\"\"\n",
    "    Callback to plot the learning curves of the model during training.\n",
    "    \"\"\"\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.metrics = {}\n",
    "        for metric in logs:\n",
    "            self.metrics[metric] = []\n",
    "            \n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        # Storing metrics\n",
    "        for metric in logs:\n",
    "            if metric in self.metrics:\n",
    "                self.metrics[metric].append(logs.get(metric))\n",
    "            else:\n",
    "                self.metrics[metric] = [logs.get(metric)]\n",
    "        \n",
    "        # Plotting\n",
    "        metrics = [x for x in logs if 'val' not in x]\n",
    "        \n",
    "        f, axs = plt.subplots(1, len(metrics), figsize=(15,5))\n",
    "        clear_output(wait=True)\n",
    "\n",
    "        for i, metric in enumerate(metrics):\n",
    "            axs[i].plot(range(1, epoch + 2), \n",
    "                        self.metrics[metric], \n",
    "                        label=metric)\n",
    "            if logs['val_' + metric]:\n",
    "                axs[i].plot(range(1, epoch + 2), \n",
    "                            self.metrics['val_' + metric], \n",
    "                            label='val_' + metric)\n",
    "                \n",
    "            axs[i].legend()\n",
    "            axs[i].grid()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# training the model\n",
    "callbacks_list = [PlotLearning()]\n",
    "model.fit(\n",
    "    dataset_train_batch,\n",
    "    validation_data=dataset_val_batch,\n",
    "    epochs=20,\n",
    "    steps_per_epoch=240,\n",
    "    validation_steps=25,\n",
    "    callbacks=callbacks_list\n",
    "    )"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# calculating the loss and accuracy on the test set.\n",
    "loss, accuracy = model.evaluate(dataset_test_batch, verbose=2, steps=25)\n",
    "print(f'Model accuracy: {accuracy}')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model.summary()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Retrieve the trained word embeddings and save them to disk"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "weights = model.get_layer('embedding').get_weights()[0]\n",
    "vocab = vectorize_layer.get_vocabulary()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "out_v = io.open('vectors.tsv', 'w', encoding='utf-8')\n",
    "out_m = io.open('metadata.tsv', 'w', encoding='utf-8')\n",
    "\n",
    "for index, word in enumerate(vocab):\n",
    "    if index == 0:\n",
    "        continue  # skip 0, it's padding.\n",
    "    vec = weights[index]\n",
    "    out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
    "    out_m.write(word + \"\\n\")\n",
    "out_v.close()\n",
    "out_m.close()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Visualize the embeddings\n",
    "To visualize the embeddings, upload them to the embedding projector.\n",
    "\n",
    "Open the [Embedding Projector](http://projector.tensorflow.org/) (this can also run in a local TensorBoard instance).\n",
    "\n",
    "Click on \"Load data\".\n",
    "\n",
    "Upload the two files you created above: vecs.tsv and meta.tsv.\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "db23a038ca476d82a9257f90bf344624541a9aeff7f66a1a2f19f0b0b997a74f"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('.venv': venv)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
